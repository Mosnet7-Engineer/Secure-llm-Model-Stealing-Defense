II . Experiment Plan
ðŸ§ª 1. Threat Model
â€¢	Attacker has:
o	Black-box API access to an LLM
o	No access to parameters
â€¢	Goal:
o	Steal the model by training a substitute using queries
________________________________________
ðŸ§ª 2. Attack Setup
You will:
â€¢	Use an open-source LLM (e.g., LLaMA-2 / Mistral / GPT-Neo) as the target
â€¢	Simulate API access
â€¢	Attacker:
o	Sends large numbers of crafted prompts
o	Collects outputs
o	Trains a student model to mimic behavior
Metrics:
â€¢	Accuracy similarity
â€¢	BLEU / ROUGE
â€¢	Embedding cosine similarity
________________________________________
ðŸ§ª 3. Defense Mechanism
Your defense has two layers:
ðŸ”¹ A. Output Watermarking
â€¢	Slightly modify word choices / token probabilities
â€¢	Embed statistical signal
â€¢	Does NOT reduce readability
ðŸ”¹ B. Query Behavior Detection
Track:
â€¢	Query rate
â€¢	Prompt diversity
â€¢	Repetition patterns
â€¢	Length distribution
Use:
â€¢	Isolation Forest / clustering / heuristics
________________________________________
ðŸ§ª 4. Evaluation Metrics
Metric	Purpose
Extraction Success Rate	How well attacker recreates model
Detection Accuracy	How often attacker is caught
False Positives	Legit users flagged
Overhead	Latency added
Utility	Output quality drop
